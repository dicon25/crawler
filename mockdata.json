{
  "tableOfContents": [
    {
      "label": "Introduction",
      "translatedLabel": "소개",
      "subContents": [
        {
          "label": "Our Results", "translatedLabel": "결과"
        },
        {
          "label": "Related Works", "translatedLabel": "관련 연구"
        }
      ]
    },
    {
      "label": "Preliminaries", "translatedLabel": "전제 조건"
    },
    {
      "label": "The Main Result", "translatedLabel": "주요 결과"
    },
    {
      "label": "The Algorithm", "translatedLabel": "알고리즘"
    }
  ],
  "contents": [
    {
      "label": "Abstract",
      "translatedLabel": "초록",
      "content": "We give a deterministic O(mlog2/3 n)-time algorithm for single-source shortest paths (SSSP) on directed graphs with real non-negative edge weights in the comparison-addition model. This is the first result to break the O(m+nlog n) time bound of Dijkstra's algorithm on sparse graphs, showing that Dijkstra's algorithm is not optimal for SSSP.",
      "translatedContent": "우리는 비교-덧셈 모델에서 실수 양수 가중치를 가진 방향 그래프의 단일 소스 최단 경로(SSSP)를 위한 결정론적 O(mlog2/3 n) 시간 알고리즘을 제시합니다. 이는 희소 그래프에서 Dijkstra 알고리즘의 O(m+nlog n) 시간 복잡도를 깨는 첫 번째 결과로, Dijkstra 알고리즘이 SSSP에 대해 최적이 아님을 보여줍니다."
    },
    {
      "label":"Introduction",
      "translatedLabel": "본문",
      "content": "Researchers from Google Brain and the University of Toronto have proposed a new natural language processing (NLP) model called 'Transformer' that will bring a revolutionary turning point to the field of artificial intelligence.\nUnlike existing recurrent neural networks (RNN) or convolutional neural networks (CNN), this model operates solely on an 'Attention' mechanism that can grasp the relationships between all words in a sentence at once without sequential computation, and is evaluated as presenting a new paradigm for AI language models.\n\nThe research team compiled their findings into a paper titled 'Attention Is All You Need' and published it on arXiv, a preprint paper site, on the 13th.\nThe paper emphasizes that the Transformer architecture provides a faster, more efficient, and parallelizable learning structure than existing models in various natural language processing tasks such as translation, summarization, and question answering.\nThis model dramatically improves contextual understanding by allowing each word in the input sentence to learn its relationship with other words through 'attention'.\n\nThe Transformer architecture presented in the paper is divided into two parts: 'Encoder' and 'Decoder', with each component centered on Multi-Head Attention and Feed-Forward Neural Networks.\nThis architecture became the foundation for large language models that emerged later, such as BERT, GPT, T5, RoBERTa, LLaMA, and Gemini, and played a decisive role in the development of today's generative AI technology.\n\nTransformer is not just a technical innovation, but is evaluated as a turning point that completely shifted the central axis of deep learning research paradigm from 'recurrence to attention'.\nIn particular, immediately after the paper was published, researchers and industries worldwide quickly adopted this architecture, and it is being widely applied in various fields such as machine translation, text generation, image caption generation, and multimodal AI.",
      "translatedContent": "구글 브레인(Google Brain)과 캐나다 토론토대학교(University of Toronto) 연구진이 인공지능 분야에 혁신적인 전환점을 가져올 새로운 자연어처리(NLP) 모델 '트랜스포머(Transformer)'를 제안했다.\n이 모델은 기존의 순환신경망(RNN)이나 합성곱신경망(CNN)과 달리 순차적 계산 없이 문장 내 모든 단어 간의 관계를 한 번에 파악할 수 있는 '어텐션(Attention)' 메커니즘만으로 작동한다는 점에서, 인공지능 언어 모델의 새로운 패러다입을 제시했다는 평가를 받고 있다.\n\n연구팀은 해당 연구 결과를 Attention Is All You Need라는 제목의 논문으로 정리해 지난 13일 사전 공개 논문 사이트인 arXiv에 게재했다.\n논문에서는 트랜스포머 구조가 번역, 요약, 질의응답 등 다양한 자연어처리 작업에서 기존 모델보다 더 빠르고, 효율적이며, 병렬화가 가능한 학습 구조를 제공한다는 점을 강조했다.\n이 모델은 입력 문장의 각 단어가 다른 단어와의 관계를 스스로 '주의(attention)'를 통해 학습함으로써, 문맥 이해 능력을 비약적으로 향상시킨다.\n\n논문에서 제시된 트랜스포머 구조는 '인코더(Encoder)'와 '디코더(Decoder)'라는 두 부분으로 나뉘며, 각 구성 요소는 멀티헤드 어텐션(Multi-Head Attention)과 피드포워드 신경망(Feed-Forward Neural Network)을 핵심으로 한다.\n이러한 구조는 이후 등장한 BERT, GPT, T5, RoBERTa, LLaMA, Gemini 등 대형 언어 모델의 기반이 되었으며, 오늘날의 생성형 AI 기술 발전에 결정적인 역할을 했다.\n\n트랜스포머는 단순히 기술적인 혁신에 그치지 않고, 딥러닝 연구 패러다임의 중심축을 '순환에서 어텐션으로'완전히 이동시킨 전화점으로 평가받고 있다.\n특히, 논문이 발표된 직후 전 세계 연구자들과 산업계가 빠르게 이 구조를 채택하며, 머신 트랜슬레이션(기계 번역), 문장 생성, 이미지 캡션 생성, 멀티모달 AI 등 다양한 분야에서 폭넓게 응용되고 있다."
    }
  ]
}